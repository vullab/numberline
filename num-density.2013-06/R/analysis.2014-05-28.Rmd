Number experiment analysis
========================================================

Experiment design
--------------------

24 subjects were presented with arrays of dots sampled from a ?? distribution.

They made two guesses about the number of dots on that trial.

Each array was presented twice (on different trials).

Read in and clean data
-------------------

(number subjects sequentially)

```{r}
setwd('~/PROJECTS/number-line/num-density.2013-06/R/')
rm(list=ls())

source('~/CODE/R/utilities.R')

files <- list.files('../data/')

dat <- data.frame()
subject <- 1
for(f in files){
  q = read.csv2(paste('../data/', f, sep=""), sep="\t", header=T, colClasses="character", skip=1)
  q$subject = subject
  dat <- rbind(dat, q)
  subject <- subject+1
}

head(dat)
```

convert numbers to numbers
```{r}
to.num <- function(x){as.numeric(as.character(x))}
dat$trial <- to.num(dat$trial)
dat$startTime <- to.num(dat$startTime)
dat$responseTime <- to.num(dat$responseTime)
dat$response <- to.num(dat$response)
dat$n <- to.num(dat$n)
dat$Rdot <- to.num(dat$Rdot)
dat$Rspace <- to.num(dat$Rspace)
dat$Nrings <- to.num(dat$Nrings)
dat$Feedback <- to.num(dat$Feedback)
dat$points2 <- to.num(dat$points)
dat$score <- to.num(dat$score)
dat$vary <- as.factor(dat$vary)
```

Basic Checks
-------------

What is the overall histogram of presented numbers?
```{r fig.width=7, fig.height=6}
hist(dat$n)
```

What is the overall RT histogram (log10 scaled seconds)?
```{r fig.width=7, fig.height=6}
hist(log10(dat$responseTime/1000))
```

For each subject, estimate RT mean and variance.
```{r fig.width=7, fig.height=6}
ms <- by(dat$responseTime/1000, dat$subject, function(x){mean(log10(x))})
vs <- by(dat$responseTime/1000, dat$subject, function(x){sd(log10(x))})
plot(ms, vs)
```

For each subject estimate the overall response-n rank correlation
```{r}
rs <- rbind(by(dat, dat$subject, 
               function(tmp){
                 cor(tmp$n, tmp$response, method="spearman")
                 }
               )
            )
hist(rs, 20)
ns <- rbind(by(dat, dat$subject, nrow))

dat <- dat[! dat$subject %in%  which(rs<0.6),]
```

Plot calibrations
-----------------

Plotting the average of the two guesses on one trial versus the actual number on that trial.

```{r fig.width=12, fig.height=8}
ggplot(dat, aes(x=n, y=response, color=vary))+
  geom_point(alpha=0.5)+
  geom_abline(position="identity")+
  mylogx(c(1,700))+
  mylogy(c(1,700))+
  xlab("Number presented")+
  ylab("Number reported")+
  mytheme+
  facet_wrap(~subject, ncol=8)
```


```{r fig.width=12, fig.height=8}
ggplot(subset(dat, dat$subject==5), aes(x=n, y=response))+
  geom_point(alpha=0.5, color="red")+
  geom_abline(position="identity")+
  scale_x_log10(limits=c(1, 600), 
                breaks = c(1, 10, 100), 
                minor_breaks=log10(c(1:10, seq(20, 100, by=10), 200, 300, 400, 500, 600)))+
  scale_y_log10(limits=c(1, 600), 
                breaks = c(1, 10, 100), 
                minor_breaks=log10(c(1:10, seq(20, 100, by=10), 200, 300, 400, 500, 600)))+
  xlab("Number presented")+
  ylab("Number reported")+
  theme_bw()+
  theme(panel.grid.minor = element_line(colour="gray90", size=0.5), 
        panel.grid.major = element_line(colour="gray", size=0.5),
        axis.text=element_text(size=12),
        axis.title=element_text(size=16,face="bold"),
        strip.background= element_rect(colour = "black", fill = "gray90",
                                          size = 0.3, linetype = "solid"),
        strip.text=element_text(size=12, face="bold"),
        panel.border=element_rect(colour="black", size=0.6))+
  facet_wrap(~vary, ncol=3)
```

What do we take away from this?  

* **Bilinear:** The functions looks "bilinear" -- calibrated for low ns, and systematically off for higher ns.  

* **Individual differences:** The systematic miscalibration appears to be different for different subjects.

* **Supra-weber variance:** Variance does not appear to be constant in the log-log plots, suggesting super-weber noise scaling.

**Let's check if variance really does scale**

Proportional error standard deviation increases with numerosity.

```{r}
qcut <- function(x, n) {
    cut(x, quantile(x, seq(0, 1, length = n + 1)), labels = seq_len(n),
        include.lowest = TRUE)
}

nqs = 10
dat$quintile <- as.factor(qcut(log10(dat$n), nqs))

s = by(log10(dat$response)-log10(dat$n), dat[,c("subject", "quintile")], sd)

vm <- apply((s[,]), 2, function(x){mean(x, na.rm=T)})
vs <- apply((s[,]), 2, function(x){sd(x, na.rm=T)/sqrt(length(x))})

cs <- quantile(dat$n, seq(0,1, length=nqs+1))
plot(vm, type='b', xlab="decile of dot number", ylab="log10(Log-Error Variance)", xaxt="n")
tmp <- lapply(1:nqs, function(i){lines(c(i,i), c(-1,1)*vs[i]+vm[i])})
axis(1, at=1:nqs, labels=paste(cs[1:nqs], cs[2:(1+nqs)], sep="-"))
```

Now let's consider time dependence
-------------------

We are going to calculate autocorrelations between presented numbers, responses, and errors.

```{r}
subjects <- unique(dat$subject)
acf.stim <- data.frame()
acf.resp <- data.frame()
acf.err <- data.frame()
nlags=20
for(s in subjects){
  tmp <- acf(subset(log10(dat$num_dots), dat$subject==s), lag.max=nlags, plot=F)
  acf.resp <- rbind(acf.resp, c(s, tmp[[1]][2:(nlags+1)]))
  tmp <- acf(subset(log10(dat$answer), dat$subject==s), lag.max=nlags, plot=F)
  acf.stim <- rbind(acf.stim, c(s, tmp[[1]][2:(nlags+1)]))
  tmp <- acf(subset(log10(dat$answer)-log10(dat$num_dots), dat$subject==s), lag.max=nlags, plot=F)
  acf.err <- rbind(acf.err, c(s, tmp[[1]][2:(nlags+1)]))
}
names(acf.resp) <- c("Subject", paste("lag.", 1:nlags, sep=''))
names(acf.stim) <- c("Subject", paste("lag.", 1:nlags, sep=''))
names(acf.err) <- c("Subject", paste("lag.", 1:nlags, sep=''))
```

Now let's make some plots

```{r fig.width=12, fig.height=8}
fish.mu <- function(x){mean(fisherz(x))}
fish.se <- function(x){sd(fisherz(x))/sqrt(length(x))}
acf.resp.mu <- apply(acf.resp[,2:(nlags+1)], 2, fish.mu)
acf.resp.se <- apply(acf.resp[,2:(nlags+1)], 2, fish.se)
acf.stim.mu <- apply(acf.stim[,2:(nlags+1)], 2, fish.mu)
acf.stim.se <- apply(acf.stim[,2:(nlags+1)], 2, fish.se)
acf.err.mu <- apply(acf.err[,2:(nlags+1)], 2, fish.mu)
acf.err.se <- apply(acf.err[,2:(nlags+1)], 2, fish.se)

plotErrorCurve <- function(mu, se, color=rgb(0,0,0)){
  for(z in c(1)){
  polygon(x=c(1:length(mu), rev(1:length(mu))), 
        y=c(ifisherz(mu-z*se), rev(ifisherz(mu+z*se))), 
        col=paste(color,"40",sep=""), lty="blank")
  }
  lines(1:length(mu), ifisherz(mu), type='b', pch=19, col=color, lwd=5)
}

plot(c(0,nlags+1), c(0,0), type='l', col='black', ylim=c(-0.02, 0.1), xlim=c(1, nlags), xlab="Lag", ylab="Autocorrelation")
plotErrorCurve(acf.err.mu, acf.err.se, rgb(0.7, 0, 0))
plotErrorCurve(acf.stim.mu, acf.stim.se, rgb(0, 0.5, 0))
plotErrorCurve(acf.resp.mu, acf.resp.se, rgb(0, 0, 1.0))
title(main="Time dependence in numerosity", sub="Autocorrelation functions for stimulus, response, error")
```

So what do we have here?

* Stimuli are not correlated over time -- as should be the case, they are pseudo-randomly chosen on each trial.

* Responses are correlated with the preceding response, but no further.

* Errors are correlated over a very long time scale.

Interim closing
--------

Here's what we have learned about the structure of responses.

* Magnitude -> number mapping does not appear to be simply logarithmic, instead it appears to be bi-linear in a log-log plot, with small numbers reasonably calibrated, while lower numbers are over/under-estimated in a manner that varies subject to subject.

* Proportional error sd increases with numerosity, suggesting super-weber scaling of errors.

* Errors are auto-correlated over many trials.



