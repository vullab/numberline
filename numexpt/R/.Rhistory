# each row, col element in X will be the log likelihood that the magnitude on the current trial maps to the column value, given the magnitude
#   and corresponding estimate in that row's trial
X = matrix(0, nrow = N_TRIALS_CALIBRATION + K_BUMPERS, ncol = MAX_ESTIMATE) # Initialize matrix for storing log likelihoods
# matrix storing proability in each col that magnitude on this trial is < magnitude in trial indicated by that row
P = matrix(rep(1 - prev.vals$p.mag.greater[1:(N_TRIALS_CALIBRATION + K_BUMPERS)], MAX_ESTIMATE), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
# matrix storing numerical estimate generated for each previous trial
B = matrix(rep(floor(prev.vals$answer_estimate[1:(N_TRIALS_CALIBRATION + K_BUMPERS)]), MAX_ESTIMATE), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
X[A <= B] = log10(P[A <= B]) # TODO if we switch this to < and the next line to >=, it makes a big diff
X[A > B] = log10(1 - P[A > B])
# across each previous trial that we've calculated a log likelihood vector for above, calculate aggregate likelihood for estimates
new.loglik = data.frame('number.est' = seq(MIN_ESTIMATE:MAX_ESTIMATE), 'sum.loglik' = colSums(X))
# process log likelihoods obtained above (add constant factor, re-convert to probability space, and normalize)
new.loglik = new.loglik %>%
mutate(prior = number.est ^ PRIOR_EXP,
prior.norm = prior / sum(prior),
sum.loglik = sum.loglik - max(sum.loglik),  #add a constant C to each value in new.loglik to make it more reasonable
loglik.probability.raw = 10 ^ sum.loglik, # reconvert out of log space
posterior.raw = loglik.probability.raw * prior.norm, # multiply loglik by the prior
posterior.norm = posterior.raw / sum(posterior.raw)) # normalize
# Different ways of generating a number estimate are each included below (sample, posterior mean, posterior median)
# 1. sample estimate from the posterior
estimate.sample = with(new.loglik, sample(number.est, 1, prob = posterior.norm))
# 2. use the posterior mean as our estimate
estimate.mean = sum(new.loglik$number.est * new.loglik$posterior.norm)
# 3. use the median as our estimate (following Griffiths, Tenenbaum 2006)
estimate.med = new.loglik %>%
mutate(post_sum = cumsum(posterior.norm)) %>%
filter(post_sum >= 0.5) %>%
summarize(est = min(number.est)) %>%
pull(est)
# 4. use MAP estimate
estimate.map = new.loglik$number.est[new.loglik$posterior.norm == max(new.loglik$posterior.norm)]
estimate = estimate.map # choose favorite here
}
# add estimate to single.subj results
single.subj$answer_estimate[single.subj$trial == trial.i] = estimate
}
# Quick view estimates and true numbers across trials
results = single.subj %>%
select(trial, num_dots, answer_estimate) %>%
arrange(trial)
results
# validating new.loglik
new.loglik %>%
#filter(number.est <= 10) %>%
ggplot(aes(x = number.est, y = posterior.norm)) +
#ggplot(aes(x = number.est, y = loglik.probability.raw)) +
geom_point() +
geom_vline(xintercept = bumper_set, color = "blue", linetype = "dashed") +
geom_vline(xintercept = estimate, color = "red", linetype = "dashed") +
#scale_x_continuous(breaks = c(1:10), labels = c(1:10)) +
labs(x = "magnitude estimate")
# validating model answers
single.subj %>%
filter(trial > N_TRIALS_CALIBRATION) %>%
ggplot(aes(x = log10(num_dots))) +
#geom_point(aes(y = log10(participant.answer), color = "sample participant"), alpha = 0.5) +
geom_point(aes(y = log10(answer_estimate)), color = "red", alpha = 0.5) +
geom_abline() +
geom_vline(xintercept = log10(bumper_set), linetype = "dashed", alpha = 0.5) +
ggtitle(paste0("Model estimates (map), calibration = ", N_TRIALS_CALIBRATION, " trial(s)")) +
labs(x = "Number presented (log10)", y = "Number estimated (log10)") +
# scale_color_manual(name = "Estimates", values = c("post. med" = "red")) +
theme(axis.title = element_text(size = 16, face = "bold"),
plot.title = element_text(size = 20, face = "bold"))
# Understanding over- and under-estimation (line segments show difference between num_dots and estimate)
single.subj %>%
#filter(trial > N_TRIALS_CALIBRATION) %>%
ggplot(aes(x = trial)) +
geom_point(aes(y = log10(num_dots), color = "true number"), alpha = 0.5) +
geom_point(aes(y = log10(answer_estimate), color = "model estimate"), alpha = 0.5) +
geom_segment(aes(y = log10(num_dots), yend = log10(answer_estimate), xend = trial)) +
geom_hline(yintercept = log10(bumper_set[bumper_set <= 10]), linetype = "dashed") +
ggtitle("Model estimates v. true numbers across trials") +
labs(y = "Number of dots (log10)", x = "trial number") +
scale_color_manual(name = "Estimates", values = c("true number" = "blue", "model estimate" = "red")) +
theme(axis.title = element_text(size = 16, face = "bold"),
plot.title = element_text(size = 20, face = "bold"))
# Select actual participant data for numbers presented, add necessary columns
single.subj = data %>%
select(subject, run, index, num_dots, answer) %>% # Note including answer only relevant for eventual comparison
rename(participant.answer = answer) %>% # rename to avoid any confusion
mutate(trial = ((run - 1) * max(index)) + index, # get true trial number (1-300) for each participant
answer_estimate = 0,
bumper_trial = FALSE) %>%
filter(subject == 17) # get single participant's data for easier initial modeling
# Globals: experiment
MIN_ESTIMATE = 1 # lowest number in the range of dots
MAX_ESTIMATE = 1000 # highest number in the range of dots
# Globals: model
PERCEIVED_DOTS_NOISE_SD = 0.01 # log noise of perceived magnitudes
# PERCEIVED_DOTS_NOISE_SD = 0.0001 # DEBUGGING
PRIOR_EXP = -1 # exponential slope parameter for prior
N_TRIALS_CALIBRATION = 50 # number of previous trials to reference in calculating answer estimate
K_BUMPERS = 5 # number of 'bumpers' participant has available to prevent over or underestimation
# matrix of number estimates initialized for use in calculation of likelihoods in loop below
A = matrix(rep(1:MAX_ESTIMATE, each = N_TRIALS_CALIBRATION + K_BUMPERS), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
# set bumpers (prior to iterating through trials)
# TODO sample from this with log sd so it's not e.g. a crystal clear bumper at 126
bumper_set = 10 ^ seq(from = log10(MIN_ESTIMATE), to = log10(MAX_ESTIMATE), length.out = K_BUMPERS)
#for (trial.i in seq(1, 49)) { # iterate to X trials past calibrated point
for (trial.i in seq(1, max(single.subj$trial))) { # over all trials
if (trial.i <= N_TRIALS_CALIBRATION) {
# set first N_TRIALS_CALIBRATION values to be a sample from a distribution around the true number of dots
# NB: samples in log space with low SD so that samples will be closer to real numbers at low n, farther at high n
# estimate = round(10 ^ rnorm(1, log10(single.subj$num_dots[trial.i]), PERCEIVED_DOTS_NOISE_SD), digits = 0)
estimate = single.subj$num_dots[trial.i] # use exact number for debugging
} else {
# fetch previous trials for magnitude comparison
comparison.trials = seq(trial.i - N_TRIALS_CALIBRATION, trial.i - 1)
prev.vals = single.subj %>%
filter(trial %in% comparison.trials)
# compare magnitude of trial.i to previous trials
curr.trial.mag.mean = single.subj$num_dots[single.subj$trial == trial.i]
prev.vals = prev.vals %>%
mutate(p.mag.greater = 1 - pnorm(0, mean = log10(curr.trial.mag.mean) - log10(num_dots), sd = sqrt(2 * PERCEIVED_DOTS_NOISE_SD)))
# add in "bumpers" as if they were previous trials, then all logic below proceeds the same
prev.vals = add_row(prev.vals,
subject = unique(prev.vals$subject),
num_dots = bumper_set,
answer_estimate = bumper_set,
bumper_trial = TRUE,
p.mag.greater = 1 - pnorm(0, mean = log10(curr.trial.mag.mean) - log10(num_dots), sd = sqrt(2 * PERCEIVED_DOTS_NOISE_SD)))
# discretize possible answer estimates, set log likelihood at each value to be either p.mag.greater or 1 - p.mag.greater
# use matrix with a row for each of the previous trials and a column for each candidate estimate value.
# each row, col element in X will be the log likelihood that the magnitude on the current trial maps to the column value, given the magnitude
#   and corresponding estimate in that row's trial
X = matrix(0, nrow = N_TRIALS_CALIBRATION + K_BUMPERS, ncol = MAX_ESTIMATE) # Initialize matrix for storing log likelihoods
# matrix storing proability in each col that magnitude on this trial is < magnitude in trial indicated by that row
P = matrix(rep(1 - prev.vals$p.mag.greater[1:(N_TRIALS_CALIBRATION + K_BUMPERS)], MAX_ESTIMATE), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
# matrix storing numerical estimate generated for each previous trial
B = matrix(rep(floor(prev.vals$answer_estimate[1:(N_TRIALS_CALIBRATION + K_BUMPERS)]), MAX_ESTIMATE), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
X[A <= B] = log10(P[A <= B]) # TODO if we switch this to < and the next line to >=, it makes a big diff
X[A > B] = log10(1 - P[A > B])
# across each previous trial that we've calculated a log likelihood vector for above, calculate aggregate likelihood for estimates
new.loglik = data.frame('number.est' = seq(MIN_ESTIMATE:MAX_ESTIMATE), 'sum.loglik' = colSums(X))
# process log likelihoods obtained above (add constant factor, re-convert to probability space, and normalize)
new.loglik = new.loglik %>%
mutate(prior = number.est ^ PRIOR_EXP,
prior.norm = prior / sum(prior),
sum.loglik = sum.loglik - max(sum.loglik),  #add a constant C to each value in new.loglik to make it more reasonable
loglik.probability.raw = 10 ^ sum.loglik, # reconvert out of log space
posterior.raw = loglik.probability.raw * prior.norm, # multiply loglik by the prior
posterior.norm = posterior.raw / sum(posterior.raw)) # normalize
# Different ways of generating a number estimate are each included below (sample, posterior mean, posterior median)
# 1. sample estimate from the posterior
estimate.sample = with(new.loglik, sample(number.est, 1, prob = posterior.norm))
# 2. use the posterior mean as our estimate
estimate.mean = sum(new.loglik$number.est * new.loglik$posterior.norm)
# 3. use the median as our estimate (following Griffiths, Tenenbaum 2006)
estimate.med = new.loglik %>%
mutate(post_sum = cumsum(posterior.norm)) %>%
filter(post_sum >= 0.5) %>%
summarize(est = min(number.est)) %>%
pull(est)
# 4. use MAP estimate
estimate.map = new.loglik$number.est[new.loglik$posterior.norm == max(new.loglik$posterior.norm)]
estimate = estimate.med # choose favorite here
}
# add estimate to single.subj results
single.subj$answer_estimate[single.subj$trial == trial.i] = estimate
}
# Quick view estimates and true numbers across trials
results = single.subj %>%
select(trial, num_dots, answer_estimate) %>%
arrange(trial)
results
# validating model answers
single.subj %>%
filter(trial > N_TRIALS_CALIBRATION) %>%
ggplot(aes(x = log10(num_dots))) +
#geom_point(aes(y = log10(participant.answer), color = "sample participant"), alpha = 0.5) +
geom_point(aes(y = log10(answer_estimate)), color = "red", alpha = 0.5) +
geom_abline() +
geom_vline(xintercept = log10(bumper_set), linetype = "dashed", alpha = 0.5) +
ggtitle(paste0("Model estimates (med), calibration = ", N_TRIALS_CALIBRATION, " trial(s)")) +
labs(x = "Number presented (log10)", y = "Number estimated (log10)") +
# scale_color_manual(name = "Estimates", values = c("post. med" = "red")) +
theme(axis.title = element_text(size = 16, face = "bold"),
plot.title = element_text(size = 20, face = "bold"))
# validating model answers
single.subj %>%
filter(trial > N_TRIALS_CALIBRATION) %>%
ggplot(aes(x = log10(num_dots))) +
geom_point(aes(y = log10(participant.answer), color = "sample participant"), alpha = 0.5) +
geom_point(aes(y = log10(answer_estimate)), color = "red", alpha = 0.5) +
geom_abline() +
geom_vline(xintercept = log10(bumper_set), linetype = "dashed", alpha = 0.5) +
ggtitle(paste0("Model estimates (med), calibration = ", N_TRIALS_CALIBRATION, " trial(s)")) +
labs(x = "Number presented (log10)", y = "Number estimated (log10)") +
# scale_color_manual(name = "Estimates", values = c("post. med" = "red")) +
theme(axis.title = element_text(size = 16, face = "bold"),
plot.title = element_text(size = 20, face = "bold"))
# validating model answers
single.subj %>%
filter(trial > N_TRIALS_CALIBRATION) %>%
ggplot(aes(x = log10(num_dots))) +
geom_point(aes(y = log10(participant.answer), color = "sample participant"), alpha = 0.5) +
geom_point(aes(y = log10(answer_estimate), color = "model"), alpha = 0.5) +
geom_abline() +
geom_vline(xintercept = log10(bumper_set), linetype = "dashed", alpha = 0.5) +
ggtitle(paste0("Model estimates (med), calibration = ", N_TRIALS_CALIBRATION, " trial(s)")) +
labs(x = "Number presented (log10)", y = "Number estimated (log10)") +
scale_color_manual(name = "Estimates", values = c("sample participant" = "blue", "model" = "red")) +
theme(axis.title = element_text(size = 16, face = "bold"),
plot.title = element_text(size = 20, face = "bold"))
# Select actual participant data for numbers presented, add necessary columns
single.subj = data %>%
select(subject, run, index, num_dots, answer) %>% # Note including answer only relevant for eventual comparison
rename(participant.answer = answer) %>% # rename to avoid any confusion
mutate(trial = ((run - 1) * max(index)) + index, # get true trial number (1-300) for each participant
answer_estimate = 0,
bumper_trial = FALSE) %>%
filter(subject == 17) # get single participant's data for easier initial modeling
# Globals: experiment
MIN_ESTIMATE = 1 # lowest number in the range of dots
MAX_ESTIMATE = 1000 # highest number in the range of dots
# Globals: model
PERCEIVED_DOTS_NOISE_SD = 0.01 # log noise of perceived magnitudes
# PERCEIVED_DOTS_NOISE_SD = 0.0001 # DEBUGGING
PRIOR_EXP = -1 # exponential slope parameter for prior
N_TRIALS_CALIBRATION = 3 # number of previous trials to reference in calculating answer estimate
K_BUMPERS = 5 # number of 'bumpers' participant has available to prevent over or underestimation
# matrix of number estimates initialized for use in calculation of likelihoods in loop below
A = matrix(rep(1:MAX_ESTIMATE, each = N_TRIALS_CALIBRATION + K_BUMPERS), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
# set bumpers (prior to iterating through trials)
# TODO sample from this with log sd so it's not e.g. a crystal clear bumper at 126
bumper_set = 10 ^ seq(from = log10(MIN_ESTIMATE), to = log10(MAX_ESTIMATE), length.out = K_BUMPERS)
#for (trial.i in seq(1, 49)) { # iterate to X trials past calibrated point
for (trial.i in seq(1, max(single.subj$trial))) { # over all trials
if (trial.i <= N_TRIALS_CALIBRATION) {
# set first N_TRIALS_CALIBRATION values to be a sample from a distribution around the true number of dots
# NB: samples in log space with low SD so that samples will be closer to real numbers at low n, farther at high n
# estimate = round(10 ^ rnorm(1, log10(single.subj$num_dots[trial.i]), PERCEIVED_DOTS_NOISE_SD), digits = 0)
estimate = single.subj$num_dots[trial.i] # use exact number for debugging
} else {
# fetch previous trials for magnitude comparison
comparison.trials = seq(trial.i - N_TRIALS_CALIBRATION, trial.i - 1)
prev.vals = single.subj %>%
filter(trial %in% comparison.trials)
# compare magnitude of trial.i to previous trials
curr.trial.mag.mean = single.subj$num_dots[single.subj$trial == trial.i]
prev.vals = prev.vals %>%
mutate(p.mag.greater = 1 - pnorm(0, mean = log10(curr.trial.mag.mean) - log10(num_dots), sd = sqrt(2 * PERCEIVED_DOTS_NOISE_SD)))
# add in "bumpers" as if they were previous trials, then all logic below proceeds the same
prev.vals = add_row(prev.vals,
subject = unique(prev.vals$subject),
num_dots = bumper_set,
answer_estimate = bumper_set,
bumper_trial = TRUE,
p.mag.greater = 1 - pnorm(0, mean = log10(curr.trial.mag.mean) - log10(num_dots), sd = sqrt(2 * PERCEIVED_DOTS_NOISE_SD)))
# discretize possible answer estimates, set log likelihood at each value to be either p.mag.greater or 1 - p.mag.greater
# use matrix with a row for each of the previous trials and a column for each candidate estimate value.
# each row, col element in X will be the log likelihood that the magnitude on the current trial maps to the column value, given the magnitude
#   and corresponding estimate in that row's trial
X = matrix(0, nrow = N_TRIALS_CALIBRATION + K_BUMPERS, ncol = MAX_ESTIMATE) # Initialize matrix for storing log likelihoods
# matrix storing proability in each col that magnitude on this trial is < magnitude in trial indicated by that row
P = matrix(rep(1 - prev.vals$p.mag.greater[1:(N_TRIALS_CALIBRATION + K_BUMPERS)], MAX_ESTIMATE), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
# matrix storing numerical estimate generated for each previous trial
B = matrix(rep(floor(prev.vals$answer_estimate[1:(N_TRIALS_CALIBRATION + K_BUMPERS)]), MAX_ESTIMATE), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
X[A <= B] = log10(P[A <= B]) # TODO if we switch this to < and the next line to >=, it makes a big diff
X[A > B] = log10(1 - P[A > B])
# across each previous trial that we've calculated a log likelihood vector for above, calculate aggregate likelihood for estimates
new.loglik = data.frame('number.est' = seq(MIN_ESTIMATE:MAX_ESTIMATE), 'sum.loglik' = colSums(X))
# process log likelihoods obtained above (add constant factor, re-convert to probability space, and normalize)
new.loglik = new.loglik %>%
mutate(prior = number.est ^ PRIOR_EXP,
prior.norm = prior / sum(prior),
sum.loglik = sum.loglik - max(sum.loglik),  #add a constant C to each value in new.loglik to make it more reasonable
loglik.probability.raw = 10 ^ sum.loglik, # reconvert out of log space
posterior.raw = loglik.probability.raw * prior.norm, # multiply loglik by the prior
posterior.norm = posterior.raw / sum(posterior.raw)) # normalize
# Different ways of generating a number estimate are each included below (sample, posterior mean, posterior median)
# 1. sample estimate from the posterior
estimate.sample = with(new.loglik, sample(number.est, 1, prob = posterior.norm))
# 2. use the posterior mean as our estimate
estimate.mean = sum(new.loglik$number.est * new.loglik$posterior.norm)
# 3. use the median as our estimate (following Griffiths, Tenenbaum 2006)
estimate.med = new.loglik %>%
mutate(post_sum = cumsum(posterior.norm)) %>%
filter(post_sum >= 0.5) %>%
summarize(est = min(number.est)) %>%
pull(est)
# 4. use MAP estimate
estimate.map = new.loglik$number.est[new.loglik$posterior.norm == max(new.loglik$posterior.norm)]
estimate = estimate.med # choose favorite here
}
# add estimate to single.subj results
single.subj$answer_estimate[single.subj$trial == trial.i] = estimate
}
# Quick view estimates and true numbers across trials
results = single.subj %>%
select(trial, num_dots, answer_estimate) %>%
arrange(trial)
results
# validating model answers
single.subj %>%
filter(trial > N_TRIALS_CALIBRATION) %>%
ggplot(aes(x = log10(num_dots))) +
geom_point(aes(y = log10(participant.answer), color = "sample participant"), alpha = 0.5) +
geom_point(aes(y = log10(answer_estimate), color = "model"), alpha = 0.5) +
geom_abline() +
geom_vline(xintercept = log10(bumper_set), linetype = "dashed", alpha = 0.5) +
ggtitle(paste0("Model estimates (med), calibration = ", N_TRIALS_CALIBRATION, " trial(s)")) +
labs(x = "Number presented (log10)", y = "Number estimated (log10)") +
scale_color_manual(name = "Estimates", values = c("sample participant" = "blue", "model" = "red")) +
theme(axis.title = element_text(size = 16, face = "bold"),
plot.title = element_text(size = 20, face = "bold"))
# Select actual participant data for numbers presented, add necessary columns
single.subj = data %>%
select(subject, run, index, num_dots, answer) %>% # Note including answer only relevant for eventual comparison
rename(participant.answer = answer) %>% # rename to avoid any confusion
mutate(trial = ((run - 1) * max(index)) + index, # get true trial number (1-300) for each participant
answer_estimate = 0,
bumper_trial = FALSE) %>%
filter(subject == 17) # get single participant's data for easier initial modeling
# Globals: experiment
MIN_ESTIMATE = 1 # lowest number in the range of dots
MAX_ESTIMATE = 1000 # highest number in the range of dots
# Globals: model
PERCEIVED_DOTS_NOISE_SD = 0.01 # log noise of perceived magnitudes
# PERCEIVED_DOTS_NOISE_SD = 0.0001 # DEBUGGING
PRIOR_EXP = -1 # exponential slope parameter for prior
N_TRIALS_CALIBRATION = 20 # number of previous trials to reference in calculating answer estimate
K_BUMPERS = 5 # number of 'bumpers' participant has available to prevent over or underestimation
# matrix of number estimates initialized for use in calculation of likelihoods in loop below
A = matrix(rep(1:MAX_ESTIMATE, each = N_TRIALS_CALIBRATION + K_BUMPERS), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
# set bumpers (prior to iterating through trials)
# TODO sample from this with log sd so it's not e.g. a crystal clear bumper at 126
bumper_set = 10 ^ seq(from = log10(MIN_ESTIMATE), to = log10(MAX_ESTIMATE), length.out = K_BUMPERS)
#for (trial.i in seq(1, 49)) { # iterate to X trials past calibrated point
for (trial.i in seq(1, max(single.subj$trial))) { # over all trials
if (trial.i <= N_TRIALS_CALIBRATION) {
# set first N_TRIALS_CALIBRATION values to be a sample from a distribution around the true number of dots
# NB: samples in log space with low SD so that samples will be closer to real numbers at low n, farther at high n
# estimate = round(10 ^ rnorm(1, log10(single.subj$num_dots[trial.i]), PERCEIVED_DOTS_NOISE_SD), digits = 0)
estimate = single.subj$num_dots[trial.i] # use exact number for debugging
} else {
# fetch previous trials for magnitude comparison
comparison.trials = seq(trial.i - N_TRIALS_CALIBRATION, trial.i - 1)
prev.vals = single.subj %>%
filter(trial %in% comparison.trials)
# compare magnitude of trial.i to previous trials
curr.trial.mag.mean = single.subj$num_dots[single.subj$trial == trial.i]
prev.vals = prev.vals %>%
mutate(p.mag.greater = 1 - pnorm(0, mean = log10(curr.trial.mag.mean) - log10(num_dots), sd = sqrt(2 * PERCEIVED_DOTS_NOISE_SD)))
# add in "bumpers" as if they were previous trials, then all logic below proceeds the same
prev.vals = add_row(prev.vals,
subject = unique(prev.vals$subject),
num_dots = bumper_set,
answer_estimate = bumper_set,
bumper_trial = TRUE,
p.mag.greater = 1 - pnorm(0, mean = log10(curr.trial.mag.mean) - log10(num_dots), sd = sqrt(2 * PERCEIVED_DOTS_NOISE_SD)))
# discretize possible answer estimates, set log likelihood at each value to be either p.mag.greater or 1 - p.mag.greater
# use matrix with a row for each of the previous trials and a column for each candidate estimate value.
# each row, col element in X will be the log likelihood that the magnitude on the current trial maps to the column value, given the magnitude
#   and corresponding estimate in that row's trial
X = matrix(0, nrow = N_TRIALS_CALIBRATION + K_BUMPERS, ncol = MAX_ESTIMATE) # Initialize matrix for storing log likelihoods
# matrix storing proability in each col that magnitude on this trial is < magnitude in trial indicated by that row
P = matrix(rep(1 - prev.vals$p.mag.greater[1:(N_TRIALS_CALIBRATION + K_BUMPERS)], MAX_ESTIMATE), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
# matrix storing numerical estimate generated for each previous trial
B = matrix(rep(floor(prev.vals$answer_estimate[1:(N_TRIALS_CALIBRATION + K_BUMPERS)]), MAX_ESTIMATE), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
X[A <= B] = log10(P[A <= B]) # TODO if we switch this to < and the next line to >=, it makes a big diff
X[A > B] = log10(1 - P[A > B])
# across each previous trial that we've calculated a log likelihood vector for above, calculate aggregate likelihood for estimates
new.loglik = data.frame('number.est' = seq(MIN_ESTIMATE:MAX_ESTIMATE), 'sum.loglik' = colSums(X))
# process log likelihoods obtained above (add constant factor, re-convert to probability space, and normalize)
new.loglik = new.loglik %>%
mutate(prior = number.est ^ PRIOR_EXP,
prior.norm = prior / sum(prior),
sum.loglik = sum.loglik - max(sum.loglik),  #add a constant C to each value in new.loglik to make it more reasonable
loglik.probability.raw = 10 ^ sum.loglik, # reconvert out of log space
posterior.raw = loglik.probability.raw * prior.norm, # multiply loglik by the prior
posterior.norm = posterior.raw / sum(posterior.raw)) # normalize
# Different ways of generating a number estimate are each included below (sample, posterior mean, posterior median)
# 1. sample estimate from the posterior
estimate.sample = with(new.loglik, sample(number.est, 1, prob = posterior.norm))
# 2. use the posterior mean as our estimate
estimate.mean = sum(new.loglik$number.est * new.loglik$posterior.norm)
# 3. use the median as our estimate (following Griffiths, Tenenbaum 2006)
estimate.med = new.loglik %>%
mutate(post_sum = cumsum(posterior.norm)) %>%
filter(post_sum >= 0.5) %>%
summarize(est = min(number.est)) %>%
pull(est)
# 4. use MAP estimate
estimate.map = new.loglik$number.est[new.loglik$posterior.norm == max(new.loglik$posterior.norm)]
estimate = estimate.map # choose favorite here
}
# add estimate to single.subj results
single.subj$answer_estimate[single.subj$trial == trial.i] = estimate
}
# Quick view estimates and true numbers across trials
results = single.subj %>%
select(trial, num_dots, answer_estimate) %>%
arrange(trial)
results
# validating model answers
single.subj %>%
filter(trial > N_TRIALS_CALIBRATION) %>%
ggplot(aes(x = log10(num_dots))) +
geom_point(aes(y = log10(participant.answer), color = "sample participant"), alpha = 0.5) +
geom_point(aes(y = log10(answer_estimate), color = "model"), alpha = 0.5) +
geom_abline() +
geom_vline(xintercept = log10(bumper_set), linetype = "dashed", alpha = 0.5) +
ggtitle(paste0("Model estimates (map), calibration = ", N_TRIALS_CALIBRATION, " trial(s)")) +
labs(x = "Number presented (log10)", y = "Number estimated (log10)") +
scale_color_manual(name = "Estimates", values = c("sample participant" = "blue", "model" = "red")) +
theme(axis.title = element_text(size = 16, face = "bold"),
plot.title = element_text(size = 20, face = "bold"))
# Select actual participant data for numbers presented, add necessary columns
single.subj = data %>%
select(subject, run, index, num_dots, answer) %>% # Note including answer only relevant for eventual comparison
rename(participant.answer = answer) %>% # rename to avoid any confusion
mutate(trial = ((run - 1) * max(index)) + index, # get true trial number (1-300) for each participant
answer_estimate = 0,
bumper_trial = FALSE) %>%
filter(subject == 17) # get single participant's data for easier initial modeling
# Globals: experiment
MIN_ESTIMATE = 1 # lowest number in the range of dots
MAX_ESTIMATE = 1000 # highest number in the range of dots
# Globals: model
PERCEIVED_DOTS_NOISE_SD = 0.01 # log noise of perceived magnitudes
# PERCEIVED_DOTS_NOISE_SD = 0.0001 # DEBUGGING
PRIOR_EXP = -1 # exponential slope parameter for prior
N_TRIALS_CALIBRATION = 10 # number of previous trials to reference in calculating answer estimate
K_BUMPERS = 5 # number of 'bumpers' participant has available to prevent over or underestimation
# matrix of number estimates initialized for use in calculation of likelihoods in loop below
A = matrix(rep(1:MAX_ESTIMATE, each = N_TRIALS_CALIBRATION + K_BUMPERS), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
# set bumpers (prior to iterating through trials)
# TODO sample from this with log sd so it's not e.g. a crystal clear bumper at 126
bumper_set = 10 ^ seq(from = log10(MIN_ESTIMATE), to = log10(MAX_ESTIMATE), length.out = K_BUMPERS)
#for (trial.i in seq(1, 49)) { # iterate to X trials past calibrated point
for (trial.i in seq(1, max(single.subj$trial))) { # over all trials
if (trial.i <= N_TRIALS_CALIBRATION) {
# set first N_TRIALS_CALIBRATION values to be a sample from a distribution around the true number of dots
# NB: samples in log space with low SD so that samples will be closer to real numbers at low n, farther at high n
# estimate = round(10 ^ rnorm(1, log10(single.subj$num_dots[trial.i]), PERCEIVED_DOTS_NOISE_SD), digits = 0)
estimate = single.subj$num_dots[trial.i] # use exact number for debugging
} else {
# fetch previous trials for magnitude comparison
comparison.trials = seq(trial.i - N_TRIALS_CALIBRATION, trial.i - 1)
prev.vals = single.subj %>%
filter(trial %in% comparison.trials)
# compare magnitude of trial.i to previous trials
curr.trial.mag.mean = single.subj$num_dots[single.subj$trial == trial.i]
prev.vals = prev.vals %>%
mutate(p.mag.greater = 1 - pnorm(0, mean = log10(curr.trial.mag.mean) - log10(num_dots), sd = sqrt(2 * PERCEIVED_DOTS_NOISE_SD)))
# add in "bumpers" as if they were previous trials, then all logic below proceeds the same
prev.vals = add_row(prev.vals,
subject = unique(prev.vals$subject),
num_dots = bumper_set,
answer_estimate = bumper_set,
bumper_trial = TRUE,
p.mag.greater = 1 - pnorm(0, mean = log10(curr.trial.mag.mean) - log10(num_dots), sd = sqrt(2 * PERCEIVED_DOTS_NOISE_SD)))
# discretize possible answer estimates, set log likelihood at each value to be either p.mag.greater or 1 - p.mag.greater
# use matrix with a row for each of the previous trials and a column for each candidate estimate value.
# each row, col element in X will be the log likelihood that the magnitude on the current trial maps to the column value, given the magnitude
#   and corresponding estimate in that row's trial
X = matrix(0, nrow = N_TRIALS_CALIBRATION + K_BUMPERS, ncol = MAX_ESTIMATE) # Initialize matrix for storing log likelihoods
# matrix storing proability in each col that magnitude on this trial is < magnitude in trial indicated by that row
P = matrix(rep(1 - prev.vals$p.mag.greater[1:(N_TRIALS_CALIBRATION + K_BUMPERS)], MAX_ESTIMATE), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
# matrix storing numerical estimate generated for each previous trial
B = matrix(rep(floor(prev.vals$answer_estimate[1:(N_TRIALS_CALIBRATION + K_BUMPERS)]), MAX_ESTIMATE), nrow = N_TRIALS_CALIBRATION + K_BUMPERS)
X[A <= B] = log10(P[A <= B]) # TODO if we switch this to < and the next line to >=, it makes a big diff
X[A > B] = log10(1 - P[A > B])
# across each previous trial that we've calculated a log likelihood vector for above, calculate aggregate likelihood for estimates
new.loglik = data.frame('number.est' = seq(MIN_ESTIMATE:MAX_ESTIMATE), 'sum.loglik' = colSums(X))
# process log likelihoods obtained above (add constant factor, re-convert to probability space, and normalize)
new.loglik = new.loglik %>%
mutate(prior = number.est ^ PRIOR_EXP,
prior.norm = prior / sum(prior),
sum.loglik = sum.loglik - max(sum.loglik),  #add a constant C to each value in new.loglik to make it more reasonable
loglik.probability.raw = 10 ^ sum.loglik, # reconvert out of log space
posterior.raw = loglik.probability.raw * prior.norm, # multiply loglik by the prior
posterior.norm = posterior.raw / sum(posterior.raw)) # normalize
# Different ways of generating a number estimate are each included below (sample, posterior mean, posterior median)
# 1. sample estimate from the posterior
estimate.sample = with(new.loglik, sample(number.est, 1, prob = posterior.norm))
# 2. use the posterior mean as our estimate
estimate.mean = sum(new.loglik$number.est * new.loglik$posterior.norm)
# 3. use the median as our estimate (following Griffiths, Tenenbaum 2006)
estimate.med = new.loglik %>%
mutate(post_sum = cumsum(posterior.norm)) %>%
filter(post_sum >= 0.5) %>%
summarize(est = min(number.est)) %>%
pull(est)
# 4. use MAP estimate
estimate.map = new.loglik$number.est[new.loglik$posterior.norm == max(new.loglik$posterior.norm)]
estimate = estimate.map # choose favorite here
}
# add estimate to single.subj results
single.subj$answer_estimate[single.subj$trial == trial.i] = estimate
}
# Quick view estimates and true numbers across trials
results = single.subj %>%
select(trial, num_dots, answer_estimate) %>%
arrange(trial)
results
# validating model answers
single.subj %>%
filter(trial > N_TRIALS_CALIBRATION) %>%
ggplot(aes(x = log10(num_dots))) +
geom_point(aes(y = log10(participant.answer), color = "sample participant"), alpha = 0.5) +
geom_point(aes(y = log10(answer_estimate), color = "model"), alpha = 0.5) +
geom_abline() +
geom_vline(xintercept = log10(bumper_set), linetype = "dashed", alpha = 0.5) +
ggtitle(paste0("Model estimates (map), calibration = ", N_TRIALS_CALIBRATION, " trial(s)")) +
labs(x = "Number presented (log10)", y = "Number estimated (log10)") +
scale_color_manual(name = "Estimates", values = c("sample participant" = "blue", "model" = "red")) +
theme(axis.title = element_text(size = 16, face = "bold"),
plot.title = element_text(size = 20, face = "bold"))
