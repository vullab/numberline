
```{r}
source('~/PROJECTS/number-line/numexpt/R/load-numexpt-data.R')

source('~/PROJECTS/number-line/numexpt/R/calculate-bin-stats.R')


```

Basic response characteristics
-------------

Let's plot (a) presented/response histograms, (b) mean/var of RT, (c)

Plot calibrations
-----------------

Overall response-presented scatter plot and bin medians.

```{r fig.width=12, fig.height=11}
xlims <- c(1, maxn)
p1 <- ggplot(dat, aes(x=num_dots, y=answer))+
  geom_point(alpha=0.07, colour=maincol, size=8)+
  geom_point(data=bin.stats, aes(x=truens, y=medians), colour=seccol, size=5)+
  geom_line(data=bin.stats, aes(x=truens, y=medians), colour=seccol, size=2)+
  geom_abline(position="identity")+
  mylogx(xlims)+
  mylogy(xlims)+
  xlab("Number presented")+
  ylab("Number reported")+
  mytheme
```


```{r  fig.width=9, fig.height=4}
xlims <- c(1, maxn)
rtplot <- ggplot(X, aes(x=truens, y=RT))+
  geom_point(colour=maincol, size=5)+
  geom_line(aes(x=truens, y=RT), colour=maincol, size=2)+
  mylogx(xlims)+
  scale_y_continuous()+
  xlab("Number presented")+
  ylab("Response time (seconds)")+
  mytheme

erplot <- ggplot(X, aes(x=truens, y=acc))+
  geom_point(colour=maincol, size=5)+
  geom_line(aes(x=truens, y=acc), colour=maincol, size=2)+
  mylogx(xlims)+
  scale_y_continuous()+
  xlab("Number presented")+
  ylab("Error proportion")+
  mytheme

multiplot(rtplot, erplot, cols=1)
```


Plotting individual subjects.

```{r fig.width=12, fig.height=8}
xlims <- c(1, maxn)
ggplot(dat, aes(x=num_dots, y=answer))+
  geom_point(alpha=0.5, colour=maincol)+
  geom_abline(position="identity")+
  mylogx(xlims)+
  mylogy(xlims)+
  xlab("Number presented")+
  ylab("Number reported")+
  mytheme+
  facet_wrap(~subject, ncol=6)
```

What do we take away from this?  

* **Bilinear:** The functions looks "bilinear" -- calibrated for low ns, and systematically off for higher ns.  

* **Individual differences:** The systematic miscalibration appears to be different for different subjects.

* **Supra-weber variance:** Variance does not appear to be constant in the log-log plots, suggesting super-weber noise scaling.

**Let's check if variance really does scale**

Proportional error standard deviation increases with numerosity.

```{r fig.width=9, fig.height=6}
qcut <- function(x, n) {
    cut(x, quantile(x, seq(0, 1, length = n + 1)), labels = seq_len(n),
        include.lowest = TRUE)
}

nqs = 10
dat$quintile <- as.factor(qcut(log10(dat$num_dots), nqs))

s = by(log10(dat$answer)-log10(dat$num_dots), dat[,c("subject", "quintile")], sd)

vm <- apply((s[,]), 2, mean)
vs <- apply((s[,]), 2, function(x){sd(x)/sqrt(length(x))})

cs <- quantile(dat$num_dots, seq(0,1, length=nqs+1))
plot(vm, type='b', col="red", lwd=2, xlab="decile of dot number", ylab="log10(Log-Error Variance)", xaxt="n")
tmp <- lapply(1:nqs, function(i){lines(c(i,i), c(-1,1)*vs[i]+vm[i], col="red", lwd=2)})
axis(1, at=1:nqs, labels=paste(cs[1:nqs], cs[2:(1+nqs)], sep="-"))

```

Now let's consider time dependence
-------------------

We are going to calculate autocorrelations between presented numbers, responses, and errors.

```{r}
subjects <- unique(dat$subject)
acf.stim <- data.frame()
acf.resp <- data.frame()
acf.err <- data.frame()
nlags=20
for(s in subjects){
  tmp <- acf(subset(log10(dat$num_dots), dat$subject==s), lag.max=nlags, plot=F)
  acf.resp <- rbind(acf.resp, c(s, tmp[[1]][2:(nlags+1)]))
  tmp <- acf(subset(log10(dat$answer), dat$subject==s), lag.max=nlags, plot=F)
  acf.stim <- rbind(acf.stim, c(s, tmp[[1]][2:(nlags+1)]))
  tmp <- acf(subset(log10(dat$answer)-log10(dat$num_dots), dat$subject==s), lag.max=nlags, plot=F)
  acf.err <- rbind(acf.err, c(s, tmp[[1]][2:(nlags+1)]))
}
names(acf.resp) <- c("Subject", paste("lag.", 1:nlags, sep=''))
names(acf.stim) <- c("Subject", paste("lag.", 1:nlags, sep=''))
names(acf.err) <- c("Subject", paste("lag.", 1:nlags, sep=''))
```

Now let's make some plots

```{r fig.width=12, fig.height=8}
acf.resp.mu <- apply(acf.resp[,2:(nlags+1)], 2, fish.mu)
acf.resp.se <- apply(acf.resp[,2:(nlags+1)], 2, fish.se)
acf.stim.mu <- apply(acf.stim[,2:(nlags+1)], 2, fish.mu)
acf.stim.se <- apply(acf.stim[,2:(nlags+1)], 2, fish.se)
acf.err.mu <- apply(acf.err[,2:(nlags+1)], 2, fish.mu)
acf.err.se <- apply(acf.err[,2:(nlags+1)], 2, fish.se)

plotErrorCurve <- function(mu, se, color=rgb(0,0,0)){
  for(z in c(1)){
  polygon(x=c(1:length(mu), rev(1:length(mu))), 
        y=c(ifisherz(mu-z*se), rev(ifisherz(mu+z*se))), 
        col=paste(color,"40",sep=""), lty="blank")
  }
  lines(1:length(mu), ifisherz(mu), type='b', pch=19, col=color, lwd=5)
}

plot(c(0,nlags+1), c(0,0), type='l', col='black', ylim=c(-0.02, 0.2), xlim=c(1, nlags), xlab="Lag", ylab="Autocorrelation")
plotErrorCurve(acf.err.mu, acf.err.se, rgb(0.7, 0, 0))
plotErrorCurve(acf.stim.mu, acf.stim.se, rgb(0, 0.5, 0))
plotErrorCurve(acf.resp.mu, acf.resp.se, rgb(0, 0, 1.0))
title(main="Time dependence in numerosity", sub="Autocorrelation functions for stimulus, response, error")
```

So what do we have here?

* Stimuli are not correlated over time -- as should be the case, they are pseudo-randomly chosen on each trial.

* Responses are correlated with the preceding response, but no further.

* Errors are correlated over a very long time scale.

Interim closing
--------

Here's what we have learned about the structure of responses.

* Magnitude -> number mapping does not appear to be simply logarithmic, instead it appears to be bi-linear in a log-log plot, with small numbers reasonably calibrated, while lower numbers are over/under-estimated in a manner that varies subject to subject.

* Proportional error sd increases with numerosity, suggesting super-weber scaling of errors.

* Errors are auto-correlated over many trials.



